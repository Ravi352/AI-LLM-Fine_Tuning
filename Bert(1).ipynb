{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TgDaYQ9-tjT_"},"outputs":[],"source":["import os\n","import math\n","import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UD-pHsf6tjUH"},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OcU6kY2stjUI"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","source":["!pip install bert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6V-GL4ztxZqk","executionInfo":{"status":"ok","timestamp":1692599261931,"user_tz":-330,"elapsed":9618,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"3a159398-9f6a-4979-ca02-ac8daa2102e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bert in /usr/local/lib/python3.10/dist-packages (2.2.0)\n","Requirement already satisfied: erlastic in /usr/local/lib/python3.10/dist-packages (from bert) (2.0.0)\n"]}]},{"cell_type":"code","source":["!pip install bert-for-tf2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bS5LFj1bxDNW","executionInfo":{"status":"ok","timestamp":1692599269616,"user_tz":-330,"elapsed":7700,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"e3de3a80-7f7e-4b5b-a6a0-42b6af35e19b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.10/dist-packages (0.14.9)\n","Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from bert-for-tf2) (0.10.2)\n","Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from bert-for-tf2) (0.8.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.23.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.66.1)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VXXfr2VExJ_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install bert-tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pIoCrSgQxnJz","executionInfo":{"status":"ok","timestamp":1692599273347,"user_tz":-330,"elapsed":3744,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"b0e8cdcf-34ea-48ba-df1d-953a8fcabbb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.10/dist-packages (1.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from bert-tensorflow) (1.16.0)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmErr5-KtjUJ","executionInfo":{"status":"error","timestamp":1692599274006,"user_tz":-330,"elapsed":673,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"b825b2bf-bbc2-48ae-cdfd-7c2c78ff6815"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-66acfb6c9f6e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModelLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStockBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_stock_config_to_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_stock_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'BertModelLayer' from 'bert' (/usr/local/lib/python3.10/dist-packages/bert/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import bert\n","from bert import BertModelLayer\n","from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n","from bert.tokenization.bert_tokenization import FullTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYqTcqK6tjUJ"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQMwYq7ztjUK"},"outputs":[],"source":["random_seed = 42\n","np.random.seed(random_seed)\n","tf.random.set_seed(random_seed)"]},{"cell_type":"markdown","metadata":{"id":"52GsxKL0tjUL"},"source":["## Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"huMmA5aJtjUO"},"outputs":[],"source":["import json\n","with open('data_full.json') as json_file:\n","    CLINC150 = json.load(json_file)\n","CLINC150_train=CLINC150['train']\n","CLINC150_test=CLINC150['test']\n","CLINC150_val=CLINC150['val']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ob2H6A8tjUQ"},"outputs":[],"source":["classes=['insurance',\n"," 'next_holiday',\n"," 'repeat',\n"," 'credit_limit_change',\n"," 'book_hotel',\n"," 'yes',\n"," 'damaged_card',\n"," 'rewards_balance',\n"," 'time',\n"," 'pto_balance',\n"," 'interest_rate',\n"," 'change_volume',\n"," 'taxes',\n"," 'sync_device',\n"," 'traffic',\n"," 'what_song',\n"," 'shopping_list',\n"," 'todo_list_update',\n"," 'order_checks',\n"," 'shopping_list_update']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NpG7QJHXtjUR"},"outputs":[],"source":["train_data=[]\n","test_data=[]\n","val_data=[]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bP8J2jG3tjUS"},"outputs":[],"source":["for c in CLINC150_train:\n","    if c[1] in classes:\n","        train_data.append(c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8B5YBxu3tjUT"},"outputs":[],"source":["for c in CLINC150_test:\n","    if c[1] in classes:\n","        test_data.append(c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mDqCDq1tjUU"},"outputs":[],"source":["for c in CLINC150_val:\n","    if c[1] in classes:\n","        val_data.append(c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_iWqa-MtjUV"},"outputs":[],"source":["df = pd.DataFrame(train_data)\n","df.to_csv('train_data.csv', index=False,header=('text','intent'))\n","train=pd.read_csv('train_data.csv')\n","print(len(train))\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHL0CVh2tjUW"},"outputs":[],"source":["df = pd.DataFrame(val_data)\n","df.to_csv('val_data.csv', index=False,header=('text','intent'))\n","valid=pd.read_csv('val_data.csv')\n","print(len(valid))\n","valid.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6hXD2dItjUX"},"outputs":[],"source":["df = pd.DataFrame(test_data)\n","df.to_csv('test_data.csv', index=False,header=('text','intent'))\n","test=pd.read_csv('test_data.csv')\n","print(len(test))\n","test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r32MJUMxtjUX"},"outputs":[],"source":["train = train.append(valid).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"II3jAlimtjUY"},"outputs":[],"source":["os.makedirs(\"model\", exist_ok=True)\n","bert_model_name=\"uncased_L-12_H-768_A-12\"\n","bert_ckpt_dir = os.path.join(\"model/\", bert_model_name)\n","bert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n","bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"]},{"cell_type":"markdown","metadata":{"id":"5myFACjXtjUY"},"source":["## Input Text Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVLLw_54tjUY"},"outputs":[],"source":["class DataPreparation:\n","\n","    text_column = \"text\"\n","    label_column = \"intent\"\n","\n","    def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):\n","        self.tokenizer = tokenizer\n","        self.max_seq_len = 0\n","        self.classes = classes\n","\n","        ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self.prepare_data, [train, test])\n","\n","        print(\"max seq_len\", self.max_seq_len)\n","        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n","        self.train_x, self.test_x = map(self.data_padding, [self.train_x, self.test_x])\n","\n","    def prepare_data(self, df):\n","        x, y = [], []\n","\n","        for _, row in tqdm(df.iterrows()):\n","            text, label = row[DataPreparation.text_column], row[DataPreparation.label_column]\n","            tokens = self.tokenizer.tokenize(text)\n","            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n","            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n","            x.append(token_ids)\n","            y.append(self.classes.index(label))\n","\n","        return np.array(x), np.array(y)\n","\n","    def data_padding(self, ids):\n","        x = []\n","        for input_ids in ids:\n","            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n","            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n","            x.append(np.array(input_ids))\n","        return np.array(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXu-urjftjUZ"},"outputs":[],"source":["tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))"]},{"cell_type":"markdown","metadata":{"id":"-BaK-GZ0tjUZ"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGFXOUYItjUZ"},"outputs":[],"source":["def model_defination(max_seq_len, bert_ckpt_file):\n","\n","    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n","        bc = StockBertConfig.from_json_string(reader.read())\n","        bert_params = map_stock_config_to_params(bc)\n","        bert_params.adapter_size = None\n","        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n","\n","    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n","    bert_output = bert(input_ids)\n","\n","    print(\"bert shape\", bert_output.shape)\n","\n","    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n","    cls_out = keras.layers.Dropout(0.5)(cls_out)\n","    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n","    logits = keras.layers.Dropout(0.5)(logits)\n","    logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n","\n","    model = keras.Model(inputs=input_ids, outputs=logits)\n","    model.build(input_shape=(None, max_seq_len))\n","\n","    load_stock_weights(bert, bert_ckpt_file)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9j4jMmf5tjUa"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2PtXpaYotjUa"},"outputs":[],"source":["classes = train.intent.unique().tolist()\n","\n","data = DataPreparation(train, test, tokenizer, classes, max_seq_len=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTz-sddptjUb"},"outputs":[],"source":["data.train_x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QNFlVvRtjUe"},"outputs":[],"source":["data.train_x[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3i6xbWkntjUf"},"outputs":[],"source":["data.train_y[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bist9000tjUg"},"outputs":[],"source":["model = model_defination(data.max_seq_len, bert_ckpt_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dX6eAf4BtjUg"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpQkF66vtjUg"},"outputs":[],"source":["model.compile(\n","  optimizer=keras.optimizers.Adam(1e-5),\n","  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXz2sjKmtjUh"},"outputs":[],"source":["log_dir = \"log/intent_detection/\" + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[-3]\n","tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n","\n","history = model.fit(\n","  x=data.train_x,\n","  y=data.train_y,\n","  validation_split=0.1,\n","  batch_size=16,\n","  shuffle=True,\n","  epochs=5,\n","  callbacks=[tensorboard_callback]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_YBF0TytjUh"},"outputs":[],"source":["_, train_acc = model.evaluate(data.train_x, data.train_y)\n","_, test_acc = model.evaluate(data.test_x, data.test_y)"]},{"cell_type":"markdown","metadata":{"id":"ePAu6aIItjUi"},"source":["## Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sEeThP9tjUi"},"outputs":[],"source":["print(\"train acc\", train_acc)\n","print(\"test acc\", test_acc)"]},{"cell_type":"markdown","metadata":{"id":"L9HyD83-tjUi"},"source":["## Let's Try"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbhFkPEPtjUi"},"outputs":[],"source":["sentences = [\n","  \"is it six o clock yet\",\n","  \"find me a hotel with good reviews in phoenix\"\n","]\n","pred_tokens = map(tokenizer.tokenize, sentences)\n","pred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n","pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n","pred_token_ids = map(\n","  lambda tids: tids +[0]*(data.max_seq_len-len(tids)),\n","  pred_token_ids\n",")\n","pred_token_ids = np.array(list(pred_token_ids))\n","predictions = model.predict(pred_token_ids).argmax(axis=-1)\n","for text, label in zip(sentences, predictions):\n","    print(\"text:\", text, \"\\nintent:\", classes[label])\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqM13EK9tjUj"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}