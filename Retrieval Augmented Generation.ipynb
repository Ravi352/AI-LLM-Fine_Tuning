{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7KtYndZ2+qn0zWUQzwTxx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQSShgjVRoc6","executionInfo":{"status":"ok","timestamp":1698985738416,"user_tz":-330,"elapsed":6632,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"afe26937-1d36-4583-ea9c-b8c611d3aaa8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.0)\n"]}],"source":["!pip install pypdf"]},{"cell_type":"code","source":["!pip install langchain"],"metadata":{"id":"LfCaihEdShcK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFLoader\n","\n","loader = PyPDFLoader(\"/content/DAZN Chatbot Conversational Designer Assessment.pdf\")\n","pages = loader.load_and_split()"],"metadata":{"id":"uhbJkuKTSdgg","executionInfo":{"status":"ok","timestamp":1698985744716,"user_tz":-330,"elapsed":18,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["pages"],"metadata":{"id":"_izrdy0xSn3h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import CharacterTextSplitter\n","\n","splitter = CharacterTextSplitter(separator=\"\\n\")\n","texts = splitter.split_text(str(pages[0]))"],"metadata":{"id":"_dHKvgZGSxKI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts"],"metadata":{"id":"Zqp5pfLmSxDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install openai"],"metadata":{"id":"BhNvXMt6Sw-R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !export OPENAI_API_KEY=\"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\"\n","openai_api_key = \"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\""],"metadata":{"id":"hK0lY21aSw6b","executionInfo":{"status":"ok","timestamp":1698985958820,"user_tz":-330,"elapsed":615,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["texts[0]"],"metadata":{"id":"4g8zwU3EdK7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install chromadb"],"metadata":{"id":"devI4mOKea0D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install openai"],"metadata":{"id":"6FvUtKgSfaxK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import TextLoader\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","import openai\n","# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n","!export OPENAI_API_KEY=\"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\"\n","embeddings = OpenAIEmbeddings(openai_api_key=\"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\")\n","raw_documents = pages\n","embeddings = OpenAIEmbeddings()\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","documents = text_splitter.split_documents(raw_documents)\n","db = Chroma.from_documents(documents,embeddings=embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"_9eu4vGEceYa","executionInfo":{"status":"error","timestamp":1698987184962,"user_tz":-330,"elapsed":682,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"5a3ec540-57b8-459f-a10f-3a76c0496c27"},"execution_count":82,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-82-8d0f4a899e9a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtext_splitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacterTextSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m    685\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0mChroma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChroma\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \"\"\"\n\u001b[0;32m--> 620\u001b[0;31m         chroma_collection = cls(\n\u001b[0m\u001b[1;32m    621\u001b[0m             \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0membedding_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Chroma.__init__() got an unexpected keyword argument 'embeddings'"]}]},{"cell_type":"code","source":["from langchain.document_loaders import TextLoader\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.vectorstores import FAISS\n","\n","# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n","db = FAISS.from_documents(documents, OpenAIEmbeddings())"],"metadata":{"id":"LJYrvGhpYYGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install PyPDF2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mIqaavnidq7","executionInfo":{"status":"ok","timestamp":1698987383643,"user_tz":-330,"elapsed":9064,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"b1dc77ce-2f84-438c-9f82-3ce3fdf95a8e"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/232.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n"]}]},{"cell_type":"code","source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","import PyPDF2\n","\n","# Define the path to your PDF file\n","pdf_file_path = 'DAZN Chatbot Conversational Designer Assessment.pdf'\n","\n","# Open the PDF file and extract the text\n","text = ''\n","with open(pdf_file_path, 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    for page_num in range(len(pdf_reader.pages)):\n","        page = pdf_reader.pages[0]\n","        text += page.extract_text()\n","\n","# Split the extracted text into chunks\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","texts = text_splitter.split_text(text)\n","\n","# Initialize OpenAI embeddings and Chroma as before\n","embeddings = OpenAIEmbeddings()\n","docsearch = Chroma.from_documents(texts, embeddings)\n","\n","# Now you can work with the extracted text from the PDF\n"],"metadata":{"id":"HBvYEZX3WeUf","colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"status":"error","timestamp":1698987507791,"user_tz":-330,"elapsed":365,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"637546a1-9c1c-4c4d-e8c7-d9ec15f1b2dc"},"execution_count":91,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-6669b54282b9>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Initialize OpenAI embeddings and Chroma as before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdocsearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Now you can work with the extracted text from the PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mChroma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChroma\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \"\"\"\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         return cls.from_texts(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mChroma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChroma\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \"\"\"\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         return cls.from_texts(\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"]}]},{"cell_type":"code","source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","import PyPDF2\n","\n","# Define a simple custom document object to encapsulate text\n","class Document:\n","    def __init__(self, page_content, metadata=None):\n","        self.page_content = page_content\n","        self.metadata = metadata\n","\n","# Define the path to your PDF file\n","pdf_file_path = 'DAZN Chatbot Conversational Designer Assessment.pdf'\n","\n","# Open the PDF file and extract the text\n","text = ''\n","with open(pdf_file_path, 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    for page_num in range(len(pdf_reader.pages)):\n","        page = pdf_reader.pages[0]\n","        text += page.extract_text()\n","\n","# Split the extracted text into chunks\n","text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0)\n","texts = text_splitter.split_text(text)\n","\n","# Create a list of custom Document objects\n","documents = [Document(page_content) for page_content in texts]\n","\n","# Initialize OpenAI embeddings and Chroma as before\n","embeddings = OpenAIEmbeddings()\n","docsearch = Chroma.from_documents(documents, embeddings)\n","\n","# Now you can work with the extracted text from the PDF\n"],"metadata":{"id":"HLP27i3IWeRt","executionInfo":{"status":"ok","timestamp":1698987811521,"user_tz":-330,"elapsed":372,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":["docsearch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHpjn_AkjwQy","executionInfo":{"status":"ok","timestamp":1698987814193,"user_tz":-330,"elapsed":723,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"78b6da88-4e80-4e48-9c7c-e104f77cf829"},"execution_count":101,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langchain.vectorstores.chroma.Chroma at 0x7a90a230ffd0>"]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["query = \"can you explain the task number 1 in the details\"\n","docs = docsearch.similarity_search(query)\n","print(docs[0].page_content)"],"metadata":{"id":"B3o3D0XBWeO-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698987832426,"user_tz":-330,"elapsed":382,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"aa786330-28a1-4e53-9a3d-e95e912aa061"},"execution_count":103,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"]},{"output_type":"stream","name":"stdout","text":["DAZN Chatbot Conversational Designer Assessment.  \n"," \n","Please find a couple of simple tasks that you can complete using any tools or software you feel is \n","appropriate . They can be submitted in any format, or a combination of different formats – whatever \n","you feel displays your capabilities best.  \n","Please use www.dazn.com  to gather information and try to follow our guidelines where possib le, but \n","if you are unsure, you can make it up  or use your best judgement and previous experiences, as a \n","Chatbot Designer or a customer!   \n","You have 48 hours to complete and return the assessment – once returned we will inform you if you \n","move onto the interv iew stages.  \n","If you have any questions please reach out to your recruiter. Good luck! 😊  \n","Task 1:  \n","Please use any design tool you ’re comfortable with (visio, miro, lucid, powerpoint etc ) and design an \n","end-to-end chatbot flow that will help a DAZN user who is having issues with a live stream – its \n","buffering, starting and stopping, or playing in low quality.  \n","Please include:  \n"," Troubleshooting steps  \n"," Final content for bot speech in English  \n"," 2 or 3 different device types – ie, web browser, phone, TV etc  \n"," No automation  \n","Task 2:  \n","Please design an other  end-to-end chatbot flow that will help a DAZN user reset their password and/or \n","recover their account if they cannot access their registered email account.  \n","Please include:  \n"," Automation  \n"," In the case of account recovery, security steps to authenticate account ownership – ie, user \n","confirms date of birth or last 4 digits of payment method.  \n","Task 3:  \n","The development team have told you they have built the above flows and they are ready for \n","deployment, please show me how you would make sure they are tested and correct  before \n","deployment.  \n","Task 4:  \n","Please tell me what KPIs you would use to measure the effi cacy of the flows you have designed above \n","and what the targets would be for each one.  \n","Include:  \n"," Business led KPIs  \n"," Customer led KPIs\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"sUE8L_KcWeMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"y0PkqY0-WeJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VX5N2-DQWeHY"},"execution_count":null,"outputs":[]}]}