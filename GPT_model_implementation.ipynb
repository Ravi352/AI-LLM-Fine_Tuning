{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMvlWhzKjY4jPiOw1XBU+fj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjnkRJuhx6TR","executionInfo":{"status":"ok","timestamp":1691072603634,"user_tz":-330,"elapsed":19471,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"f6d5e55d-b285-444c-ffc3-ef8c3b714ff6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Z688gZBxrfT","executionInfo":{"status":"ok","timestamp":1691072700584,"user_tz":-330,"elapsed":15926,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"0dee9487-355d-4771-8743-64bae469f01a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great\n"]}],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load pre-trained GPT-2 model and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# Generate text\n","prompt_text = \"Once upon a time\"\n","input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n","output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n","\n","# Convert the output ids back to text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(generated_text)\n"]},{"cell_type":"code","source":["import random\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","def generate_sentence(prompt_text, num_sentences=1, max_length=50, temperature=1.0):\n","    # Load pre-trained GPT-2 model and tokenizer\n","    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","    model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","    # Encode the prompt\n","    input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n","\n","    # Generate text\n","    generated_texts = []\n","    for _ in range(num_sentences):\n","        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1,\n","                                pad_token_id=tokenizer.eos_token_id, temperature=temperature)\n","        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","        generated_texts.append(generated_text)\n","\n","    return generated_texts\n","\n","# Prompt for generating new sentences\n","prompt_text = \"Once upon a time\"\n","\n","# Generate one sentence with a max length of 50 tokens and a temperature of 1.0\n","generated_sentences = generate_sentence(prompt_text, num_sentences=1, max_length=50, temperature=1.0)\n","\n","# Print the generated sentence(s)\n","print(generated_sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"moSLE-XUzGYm","executionInfo":{"status":"ok","timestamp":1691073185807,"user_tz":-330,"elapsed":8555,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"d89571a5-6a83-4c71-88da-d712ce4e01ba"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["['Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a']\n"]}]}]}