{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP40nnUGgY0Vsq3pWxRtwC5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"RsoAxOH4MsIQ","executionInfo":{"status":"ok","timestamp":1699250179059,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"outputs":[],"source":["!mkdir cache\n","!pip install -U accelerate --quiet\n","!pip install pyspark --quiet\n","!pip install delta-spark --quiet\n","!pip install mlflow --quiet\n","!pip install pyngrok --quiet"]},{"cell_type":"code","source":["!pip install datasets\n","!pip install transformers\n"],"metadata":{"id":"fRZCz1HhOqmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import pipeline"],"metadata":{"id":"VDkQ60LxNDli","executionInfo":{"status":"ok","timestamp":1699250715761,"user_tz":-330,"elapsed":14467,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"M1RO4ppiVcG-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xsum_dataset = load_dataset(\n","    \"xsum\", version=\"1.2.0\", cache_dir=\"cache\"\n",")  # Note: We specify cache_dir to use pre-cached data.\n","xsum_sample = xsum_dataset[\"train\"].select(range(10))\n","display(xsum_sample.to_pandas())"],"metadata":{"id":"Z4SuPqcdNDil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xsum_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmTaivu5VdwW","executionInfo":{"status":"ok","timestamp":1699252399429,"user_tz":-330,"elapsed":452,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"74ee156d-02dd-429b-9bc0-88455a9f709c"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 204045\n","    })\n","    validation: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 11332\n","    })\n","    test: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 11334\n","    })\n","})"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["import pyspark\n","from delta import *\n","\n","builder = pyspark.sql.SparkSession.builder.appName(\"LLMOps\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n","\n","spark = configure_spark_with_delta_pip(builder).getOrCreate()"],"metadata":{"id":"Q2MF-3bYNDfj","executionInfo":{"status":"ok","timestamp":1699250882425,"user_tz":-330,"elapsed":11942,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["prod_data_path = \"cache/m6_prod_data\"\n","test_spark_dataset = spark.createDataFrame(xsum_dataset[\"test\"].to_pandas())\n","test_spark_dataset.write.format(\"delta\").mode(\"overwrite\").save(prod_data_path)"],"metadata":{"id":"T-mvxFPVNDc9","executionInfo":{"status":"ok","timestamp":1699250986338,"user_tz":-330,"elapsed":41301,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Later, we plan to log all of these parameters to MLflow.\n","# Storing them as variables here will help with that.\n","hf_model_name = \"t5-small\"\n","min_length = 20\n","max_length = 100\n","truncation = True\n","do_sample = True\n","\n","summarizer = pipeline(\n","    task=\"summarization\",\n","    model=hf_model_name,\n","    min_length=min_length,\n","    max_length=max_length,\n","    truncation=truncation,\n","    do_sample=do_sample,\n","    model_kwargs={\"cache_dir\": \"cache\"},\n",")"],"metadata":{"id":"ASUvHB5iNDaZ","executionInfo":{"status":"ok","timestamp":1699251147450,"user_tz":-330,"elapsed":1967,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["doc0 = xsum_sample[\"document\"][0]\n","doc0"],"metadata":{"id":"w0DfVO8XNDW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summarizer(doc0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Izmgsa2RCMp","executionInfo":{"status":"ok","timestamp":1699251251959,"user_tz":-330,"elapsed":8828,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"a4193b50-4b32-4fc1-c30d-cb31575ed6d7"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'summary_text': 'the full cost of damage in Newton Stewart is still being assessed . many roads in peeblesshire remain badly affected by standing water . the water breached a retaining wall, flooding many commercial properties .'}]"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["print(f\"Summary: {summarizer(doc0)[0]['summary_text']}\")\n","print(\"===============================================\")\n","print(f\"Original Document: {doc0}\")"],"metadata":{"id":"-paX203eRCJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","results = summarizer(xsum_sample[\"document\"])\n","display(pd.DataFrame(results, columns=[\"summary_text\"]))"],"metadata":{"id":"zUUya1kkNDUH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import mlflow\n","import mlflow.pytorch"],"metadata":{"id":"NKB0lo4lNDOC","executionInfo":{"status":"ok","timestamp":1699252582384,"user_tz":-330,"elapsed":2250,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n","mlflow.pytorch.autolog()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"felL36EPNDLJ","executionInfo":{"status":"ok","timestamp":1699252834934,"user_tz":-330,"elapsed":424,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"470cf915-0b26-480b-9c33-86507e782e2e"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["2023/11/06 06:40:34 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","from getpass import getpass\n","\n","# Terminate open tunnels if exist\n","ngrok.kill()\n","\n","# Setting the authtoken (optional)\n","# Get your authtoken from https://dashboard.ngrok.com/auth\n","#NGROK_AUTH_TOKEN = \"2Padn9VzXvPy7nJXe6eAUTR3Dbd_6cXCwQeLNLwZDCWL5ypKs\"\n","#ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n","\n","# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n","ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n","print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TR7AapywNDIo","executionInfo":{"status":"ok","timestamp":1699253199885,"user_tz":-330,"elapsed":376,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"87a1c0ee-50ef-4898-f137-b35db5fe3e04"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pyngrok.process.ngrok:t=2023-11-06T06:46:39+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n","WARNING:pyngrok.process.ngrok:t=2023-11-06T06:46:39+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"]},{"output_type":"stream","name":"stdout","text":["MLflow Tracking UI: https://3ece-34-135-110-177.ngrok.io\n"]}]},{"cell_type":"code","source":["# Tell MLflow Tracking to user this explicit experiment path,\n","# which is in your home directory under the Workspace browser (left-hand sidebar).\n","mlflow.set_experiment(\"MLflow experiment\")\n","\n","with mlflow.start_run():\n","    # LOG PARAMS\n","    mlflow.log_params(\n","        {\n","            \"hf_model_name\": hf_model_name,\n","            \"min_length\": min_length,\n","            \"max_length\": max_length,\n","            \"truncation\": truncation,\n","            \"do_sample\": do_sample,\n","        }\n","    )\n","\n","    # --------------------------------\n","    # LOG INPUTS (QUERIES) AND OUTPUTS\n","    # Logged `inputs` are expected to be a list of str, or a list of str->str dicts.\n","    results_list = [r[\"summary_text\"] for r in results]\n","\n","    # Our LLM pipeline does not have prompts separate from inputs, so we do not log any prompts.\n","    mlflow.llm.log_predictions(\n","        inputs=xsum_sample[\"document\"],\n","        outputs=results_list,\n","        prompts=[\"\" for _ in results_list],\n","    )\n","\n","    # ---------\n","    # LOG MODEL\n","    # We next log our LLM pipeline as an MLflow model.\n","    # This packages the model with useful metadata, such as the library versions used to create it.\n","    # This metadata makes it much easier to deploy the model downstream.\n","    # Under the hood, the model format is simply the ML library's native format (Hugging Face for us), plus metadata.\n","\n","    # It is valuable to log a \"signature\" with the model telling MLflow the input and output schema for the model.\n","    signature = mlflow.models.infer_signature(\n","        xsum_sample[\"document\"][0],\n","        mlflow.transformers.generate_signature_output(\n","            summarizer, xsum_sample[\"document\"][0]\n","        ),\n","    )\n","    print(f\"Signature:\\n{signature}\\n\")\n","\n","    # For mlflow.transformers, if there are inference-time configurations,\n","    # those need to be saved specially in the log_model call (below).\n","    # This ensures that the pipeline will use these same configurations when re-loaded.\n","    inference_config = {\n","        \"min_length\": min_length,\n","        \"max_length\": max_length,\n","        \"truncation\": truncation,\n","        \"do_sample\": do_sample,\n","    }\n","\n","    # Logging a model returns a handle `model_info` to the model metadata in the tracking server.\n","    # This `model_info` will be useful later in the notebook to retrieve the logged model.\n","    model_info = mlflow.transformers.log_model(\n","        transformers_model=summarizer,\n","        artifact_path=\"summarizer\",\n","        task=\"summarization\",\n","        inference_config=inference_config,\n","        signature=signature,\n","        input_example=\"This is an example of a long news article which this pipeline can summarize for you.\",\n","    )"],"metadata":{"id":"MBHvMCO1NDF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_summarizer = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n","loaded_summarizer.predict(xsum_sample[\"document\"][0])"],"metadata":{"id":"RA-cu0r2NDCn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = loaded_summarizer.predict(xsum_sample.to_pandas()[\"document\"])\n","display(pd.DataFrame(results, columns=[\"generated_summary\"]))"],"metadata":{"id":"lrp430OPNC9i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the name for the model in the Model Registry.\n","# We filter out some special characters which cannot be used in model names.\n","model_name = \"summarizer - RaviKumar\"\n","model_name = model_name.replace(\"/\", \"_\").replace(\".\", \"_\").replace(\":\", \"_\")\n","print(model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XF5sTWB3NC5s","executionInfo":{"status":"ok","timestamp":1699253904976,"user_tz":-330,"elapsed":479,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"0adf2687-c96f-4032-80f3-fa0283e25179"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["summarizer - RaviKumar\n"]}]},{"cell_type":"code","source":["mlflow.register_model(model_uri=model_info.model_uri, name=model_name)"],"metadata":{"id":"aYmTKNr9NC3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mlflow import MlflowClient\n","\n","client = MlflowClient()"],"metadata":{"id":"yc-GAWb4NC0i","executionInfo":{"status":"ok","timestamp":1699253951488,"user_tz":-330,"elapsed":346,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["client.search_registered_models(filter_string=f\"name = '{model_name}'\")"],"metadata":{"id":"PMfNgqc5NCx5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_version = 1\n","dev_model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n","dev_model"],"metadata":{"id":"zP5VPv0bNCuI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["client.transition_model_version_stage(model_name, model_version, \"staging\")"],"metadata":{"id":"W-vR00yRNCqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["staging_model = dev_model"],"metadata":{"id":"zoWQnMSRNCnR","executionInfo":{"status":"ok","timestamp":1699254044359,"user_tz":-330,"elapsed":364,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["results = staging_model.predict(xsum_sample.to_pandas()[\"document\"])\n","display(pd.DataFrame(results, columns=[\"generated_summary\"]))"],"metadata":{"id":"FQ8F5io-NCkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xUdiyz3JNCgf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NdrOUZagNCIf"},"execution_count":null,"outputs":[]}]}